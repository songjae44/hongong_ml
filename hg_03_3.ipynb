{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMxRQB/eG0Cm29m9fTuCNzU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["여러개의 특성을 사용한 선형 회귀를 ***다중 회귀***라고함\n","- 특성이 1개면 직선을, 특성이 2개면 평면을 학습함\n","\n","기존의 특성을 사용하여 새로운 특성을 뽑아내는 작업을 ***특성 공학***이라고 함\n","\n","특성이 늘어났기 때문에 복붙보단 데이터를 다운로드 -> pandas사용\n","\n","dataframe은 pandas의 핵심 데이터 구조\n","\n","dataframe은 넘파이 배열보다 더 많은 기능 제공, 넘파이 배열로 쉽게 변경 가능"],"metadata":{"id":"CQdSKGIcY1mT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJS5CI9hMfhk"},"outputs":[],"source":["# 데이터 준비\n","# https://bit.ly/perch_csv_data 에 접속하여 데이터 다운로드\n","\n","# pandas 호출\n","import pandas as pd\n","\n","# 데이터 다운로드하여 df로 저장\n","df = pd.read_csv('https://bit.ly/perch_csv_data')\n","\n","# df를 numpy배열로 변경\n","perch_full = df.to_numpy()\n","\n","print(perch_full)"]},{"cell_type":"code","source":["# 타깃 데이터는 이전과 동일한 방식으로 준비\n","# https://bit.ly/perch_data 에서 복붙\n","\n","# numpy 호출\n","import numpy as np\n","\n","# 타깃 데이터 준비\n","perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,\n","       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,\n","       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,\n","       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,\n","       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,\n","       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,\n","       1000.0])"],"metadata":{"id":"hJS0t0uuatNU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# perch_full과 perch_weight을 훈련 세트와 테스트 세트로 분리\n","from sklearn.model_selection import train_test_split\n","\n","train_input, test_input, train_target, test_target = train_test_split(\n","    perch_full, perch_weight, random_state=42)"],"metadata":{"id":"yAkERndfbJ5-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["사이킷런에서는 특성을 만들거나 전처리하기 위한 다양한 클래스 제공\n","\n","이러한 클래스를 ***변환기***라고 부름\n","\n","변환기 클래스는 모두 fit(), score() 메서드 제공"],"metadata":{"id":"DVFcfOo7bg3m"}},{"cell_type":"code","source":["# PolynomialFeatures 클래스 사용\n","# sklearn.preprocessing 패키지에 포함\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# 변환기 객체 생성\n","poly = PolynomialFeatures()\n","\n","poly.fit([[2,3]])\n","print(poly.transform([[2,3]]))"],"metadata":{"id":"35UZclQmbcMr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PolynomialFeatures는 기본적으로 각 특성의 제곱한 항을 추가하고 특성끼리 서로 곱한 항을 추가\n","\n","$\n","무게 = a * 길이 + b * 높이 + c * 두께 + 1\n","$\n","\n","선형 방정식의 절편을 항상 1인 특성과 곱해지는 계수라고 볼 수 있음\n","\n","이렇게 놓고 보면 특성은 (길이,높이,두께.1)이 된다\n","\n","사이킷런의 선형 모델은 자동으로 절편을 추가 하므로 특성을 만들 필요가 없다\n","\n","include_bias=False로 지정하여 절편 없이 생성 가능(지정하지 않아도 사이킷런 모델은 자동으로 특성에 추가된 절편 항을 무시하므로 필수는 아님)"],"metadata":{"id":"QA_v1x6kci-B"}},{"cell_type":"code","source":["#include_bias=False 추가하여 다시 생성\n","poly = PolynomialFeatures(include_bias=False)\n","poly.fit([[2,3]])\n","print(poly.transform([[2,3]]))\n","# 절편을 위한 항이 제거되고 특성의 제공과 특성끼리 곱한 항만 추가"],"metadata":{"id":"BYgwTefUcAT-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이 방식으로 train_input에 적용\n","# train_input을 변환한 데이터를 train_poly에 저장하고 배열 크기 확인\n","poly = PolynomialFeatures(include_bias=False)\n","poly.fit(train_input)\n","train_poly = poly.transform(train_input)\n","print(train_poly.shape)"],"metadata":{"id":"oPKlNJ2ydX6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get_features_names_out()메서드 호출하여 특성 확인\n","poly.get_feature_names_out()"],"metadata":{"id":"m1q4pPTWd20s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 세트 변환\n","test_poly = poly.transform(test_input)"],"metadata":{"id":"FLRgM-LReDtW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LinearRegression 클래스 호출하고 train_poly사용하여 모델 훈련\n","from sklearn.linear_model import LinearRegression\n","\n","lr = LinearRegression()\n","lr.fit(train_poly, train_target)"],"metadata":{"id":"M7ltF5CEeMFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 점수 출력\n","print(lr.score(train_poly, train_target))\n","print(lr.score(test_poly, test_target))"],"metadata":{"id":"i7FCYCc4ed42"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PolynomialFeatures클래스의 degree 매개변수로 고차항의 최대 차수 지정가능\n","poly = PolynomialFeatures(degree=5, include_bias=False)\n","poly.fit(train_input)\n","train_poly = poly.transform(train_input)\n","test_poly = poly.transform(test_input)\n","print(train_poly.shape)"],"metadata":{"id":"SJebKlSdeitg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 선형회귀 모델로 다시 학습\n","lr.fit(train_poly, train_target)\n","print(lr.score(train_poly, train_target))\n","print(lr.score(test_poly, test_target))"],"metadata":{"id":"8IjW3oVDe0lh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***규제***는 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는 것을 말함\n","\n","즉, 모델이 훈련 세트에 과대적합되지 않도록 만드는 것\n","- 선형 회귀 모델의 경우 특성에 곱해지는 계수(또는 기울기)의 크기를 작게 만드는 일\n","\n","일반적으로 선형 회귀 모델에 규제를 적용할 때 계수 값의 크기가 서로 많이 다르면 공정하게 제어되지 않을 수 있음\n","- 규제 전에 먼저 정규화가 필요\n","- 사이킷런에서 제공하는 StandardScaler 클래스 사용\n"],"metadata":{"id":"Po9anmRBfDhA"}},{"cell_type":"code","source":["# StandardScaler 호출\n","from sklearn.preprocessing import StandardScaler\n","\n","ss = StandardScaler()\n","ss.fit(train_poly)\n","\n","# 꼭 훈련 세트로 학습한 변환기를 사용해 테스트 세트까지 변환해야함\n","train_scaled = ss.transform(train_poly)\n","test_scaled = ss.transform(test_poly)"],"metadata":{"id":"94s1IR4ue8GY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["선형 회귀 모델에 규제를 추가한 모델을 ***릿지***와 ***라쏘***라고 부름\n","- 릿지는 계수를 제곱한 값을 기준으로 규제 적용\n","- 라쏘는 계수의 절댓값을 기준으로 규제 적용\n","- 일반적으로 릿지를 조금 더 선호"],"metadata":{"id":"po61X4Xkgz0h"}},{"cell_type":"code","source":["# 릿지 회귀\n","from sklearn.linear_model import Ridge\n","\n","ridge = Ridge()\n","ridge.fit(train_scaled, train_target)\n","print(ridge.score(train_scaled, train_target))\n","print(ridge.score(test_scaled, test_target))"],"metadata":{"id":"SUwlm3OQgxZs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["릿지와 라쏘 모델을 사용할 때 규제의 양을 임의로 조절 가능\n","- 모델 객체를 만들 때 alpha 매개변수로 규제의 강도 조절\n","- alpha값이 크면 규제 강도가 세지므로 계수 값을 더 줄이고 조금 더 과소적합되도록 유도\n","- alpha값이 작으면 계수를 줄이는 역할이 줄어들고 선형 회귀 모델과 유사해지므로 과대적합할 가능성이 큼\n","- alpha 값은 사전에 사람이 지정해야 하는 값\n","    - 이러한 머신러닝 모델이 학습할 수 없고 사람이 알려줘야 하는 파라미터를 ***하이퍼파라미터***라고 함"],"metadata":{"id":"I5JTZv5OhW-I"}},{"cell_type":"code","source":["# 적절한 alpha 값을 찾는 법은 alpha 값에 대한 R^2 값의 그래프를 그려 보는 것\n","# 훈련 세트와 테스트 세트의 점수가 가장 가까운 지점이 최적의 alpha\n","import matplotlib.pyplot as plt\n","\n","# alpha값을 바꿀 때마다 score() 메서드의 결과를 저장할 리스트 생성\n","train_score = []\n","test_score = []"],"metadata":{"id":"ovucvL0yhPd2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# alpha값을 0.001에서 100까지 10배씩 늘려가며 릿지 회귀 모델 훈련\n","# 훈련 세트와 테스트 세트의 점수를 파이썬 리스트에 저장\n","alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]\n","for alpha in alpha_list:\n","    # 릿지 모델 생성\n","    ridge = Ridge(alpha=alpha)\n","    # 릿지 모델 훈련\n","    ridge.fit(train_scaled, train_target)\n","    # 훈련 점수와 테스트 점수를 저장\n","    train_score.append(ridge.score(train_scaled, train_target))\n","    test_score.append(ridge.score(test_scaled, test_target))"],"metadata":{"id":"h581rTaRiYAj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# np.log10 로그 함수 사용하여 6개의 값을 동일한 간격으로 나타냄\n","plt.plot(np.log10(alpha_list), train_score)\n","plt.plot(np.log10(alpha_list), test_score)\n","plt.xlabel('alpha')\n","plt.ylabel('R^2')\n","plt.show()\n","\n","# 두 그래프가 가장 가까운 지점인 -1, 즉 10^-1=0.1이 최적의 alpha값"],"metadata":{"id":"DH4E0zSAi3FO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# alpha=0.1로 최종 모델을 훈련\n","ridge = Ridge(alpha=0.1)\n","ridge.fit(train_scaled, train_target)\n","print(ridge.score(train_scaled, train_target))\n","print(ridge.score(test_scaled, test_target))"],"metadata":{"id":"uOq0nUNUjMmL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 라쏘 회귀\n","# Ridge 클래스를 Lasso 클래스로 바꾸는 것\n","from sklearn.linear_model import Lasso\n","\n","lasso = Lasso()\n","lasso.fit(train_scaled, train_target)\n","print(lasso.score(train_scaled, train_target))"],"metadata":{"id":"SU2Zdr3hjgy8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 점수 확인\n","print(lasso.score(test_scaled, test_target))"],"metadata":{"id":"zLU8HGqcjvum"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 라쏘 모델도 alpha 값을 바꾸어 가면 최적 값 도출\n","train_score = []\n","test_score = []\n","alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]\n","for alpha in alpha_list:\n","    # 라쏘 모델 생성\n","    lasso = Lasso(alpha=alpha, max_iter=10000)\n","    # 라쏘 모델 훈련\n","    lasso.fit(train_scaled, train_target)\n","    # 훈련 점수와 테스트 점수를 저장\n","    train_score.append(lasso.score(train_scaled, train_target))\n","    test_score.append(lasso.score(test_scaled, test_target))"],"metadata":{"id":"hZoVcXfwjz33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_score와 test_score를 사용해 그래프 플롯\n","plt.plot(np.log10(alpha_list), train_score)\n","plt.plot(np.log10(alpha_list), test_score)\n","plt.xlabel('alpha')\n","plt.ylabel('R^2')\n","plt.show()\n","\n","# 최적의 alpha값은 1, 즉 10^1=10"],"metadata":{"id":"SDreqAFtkI7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 최적 alpha값으로 최종 모델 훈련\n","lasso = Lasso(alpha=10)\n","lasso.fit(train_scaled, train_target)\n","print(lasso.score(train_scaled, train_target))\n","print(lasso.score(test_scaled, test_target))"],"metadata":{"id":"gZPzdgeJkUBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lasso모델의 계수는 coef_ 속성에 저장되어 있음\n","# 이 중에 0인 것 찾기\n","print(np.sum(lasso.coef_ == 0))\n","\n","# 총 55개의 특성 중 15개만 사용\n","# 라쏘모델은 유용한 특성을 골라내는 용도로도 사용 가능"],"metadata":{"id":"WtSLGNdBkgxS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SPPysD1Vkwrx"},"execution_count":null,"outputs":[]}]}